{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U kaggle_environments"
      ],
      "metadata": {
        "id": "gCSc74Jowh2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_environments import make\n",
        "\n",
        "# Создаем окружение для игры\n",
        "env = make(\"rps\", configuration={\"episodeSteps\": 100})\n",
        "\n",
        "# Агент, который всегда выбирает \"камень\"\n",
        "def rock_agent(observation, configuration):\n",
        "    return 0  # 0 означает \"камень\"\n",
        "\n",
        "# Агент, который делает случайный выбор\n",
        "import random\n",
        "def random_agent(observation, configuration):\n",
        "    return random.randint(0, 2)  # 0, 1, 2: камень, бумага, ножницы\n",
        "\n",
        "# Запуск игры между двумя агентами\n",
        "env.run([rock_agent, random_agent])\n",
        "\n",
        "# Визуализация результатов\n",
        "env.render(mode=\"ipython\")"
      ],
      "metadata": {
        "id": "HHIVobXVw0FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Разработка собственных агентов - ботов (15 стратегий)**\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBMVn3yFxCmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Агент, который всегда выбирает \"камень\"\n",
        "def rock_agent(observation, configuration):\n",
        "    return 0  # Камень\n",
        "\n",
        "# 2. Агент, который всегда выбирает \"ножницы\"\n",
        "def scissors_agent(observation, configuration):\n",
        "    return 2  # Ножницы\n",
        "\n",
        "# 3. Агент, который всегда выбирает \"бумагу\"\n",
        "def paper_agent(observation, configuration):\n",
        "    return 1  # Бумага\n",
        "\n",
        "# 4. Агент, который делает случайный выбор\n",
        "def random_agent(observation, configuration):\n",
        "    return random.randint(0, 2)  # Случайный выбор: камень, бумага, ножницы\n",
        "\n",
        "# 5. Агент, который зеркалирует предыдущий ход противника\n",
        "def mirror_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return random.randint(0, 2)  # В первый ход случайный выбор\n",
        "    else:\n",
        "        return observation.lastOpponentAction  # Повторяет ход противника\n",
        "\n",
        "# 6. Агент, который всегда контратакует прошлый ход противника\n",
        "def counter_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return random.randint(0, 2)\n",
        "    else:\n",
        "        # Всегда выбирает ход, который побеждает предыдущий ход противника\n",
        "        return (observation.lastOpponentAction + 1) % 3\n",
        "\n",
        "# 7. Агент, который чаще выбирает камень, но иногда меняет\n",
        "def biased_random_agent(observation, configuration):\n",
        "    return random.choices([0, 1, 2], weights=[0.6, 0.2, 0.2])[0]  # Чаще выбирает камень\n",
        "\n",
        "# 8. Агент, который чередует ходы камень, бумага, ножницы\n",
        "def pattern_agent(observation, configuration):\n",
        "    return observation.step % 3  # Камень -> Бумага -> Ножницы\n",
        "\n",
        "# 9. Агент, который повторяет первый ход противника\n",
        "def copy_first_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return random.randint(0, 2)\n",
        "    else:\n",
        "        return observation['actionHistories'][1][0]  # Копирует первый ход противника\n",
        "\n",
        "# 10. Агент, который меняет стратегию при проигрыше\n",
        "def adaptive_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return random.randint(0, 2)\n",
        "    else:\n",
        "        if observation.reward < 0:  # Проигрыш\n",
        "            return (observation.lastOpponentAction + 1) % 3  # Меняем на другой ход\n",
        "        else:\n",
        "            return observation.lastOpponentAction  # Если выиграл, повторяет\n",
        "\n",
        "# 11. Агент, который дважды выбирает камень, а затем случайно\n",
        "def rock_twice_then_random_agent(observation, configuration):\n",
        "    if observation.step < 2:\n",
        "        return 0  # Первые два хода \"камень\"\n",
        "    return random.randint(0, 2)  # Дальше случайный выбор\n",
        "\n",
        "# 12. Агент, который повторяет выигрышный ход\n",
        "def repeat_winner_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return random.randint(0, 2)\n",
        "    if observation.reward > 0:  # Если выиграл, повторяет ход\n",
        "        return observation.lastOpponentAction\n",
        "    else:\n",
        "        return random.randint(0, 2)  # Если проиграл, случайный выбор\n",
        "\n",
        "# 13. Агент, который использует последние два хода для анализа\n",
        "def last_two_moves_agent(observation, configuration):\n",
        "    if observation.step < 2:\n",
        "        return random.randint(0, 2)\n",
        "    # Анализируем последние два хода противника\n",
        "    last_two = [observation['actionHistories'][1][-1], observation['actionHistories'][1][-2]]\n",
        "    return (sum(last_two) % 3)  # Как-то обрабатываем последние два хода\n",
        "\n",
        "# 14. Агент, который чередует камень и ножницы\n",
        "def alternate_rock_scissors_agent(observation, configuration):\n",
        "    if observation.step % 2 == 0:\n",
        "        return 0  # Камень\n",
        "    else:\n",
        "        return 2  # Ножницы\n",
        "\n",
        "# 15. Агент, который копирует предыдущий ход противника, но чередует с камнем\n",
        "def tit_for_tat_agent(observation, configuration):\n",
        "    if observation.step == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return (observation.lastOpponentAction + observation.step) % 3"
      ],
      "metadata": {
        "id": "9SpAbXZnw-GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Список агентов для турнира\n",
        "agents = [\n",
        "    rock_agent, scissors_agent, paper_agent, random_agent,\n",
        "    mirror_agent, counter_agent, biased_random_agent,\n",
        "    pattern_agent, copy_first_agent, adaptive_agent,\n",
        "    rock_twice_then_random_agent, repeat_winner_agent,\n",
        "    last_two_moves_agent, alternate_rock_scissors_agent,\n",
        "    tit_for_tat_agent\n",
        "]\n"
      ],
      "metadata": {
        "id": "4BH7fuMpyHez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Запуск турнира между агентами**"
      ],
      "metadata": {
        "id": "lkw0GM7qxepC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск турнира: каждый агент играет с каждым\n",
        "for i in range(len(agents)):\n",
        "    for j in range(i + 1, len(agents)):\n",
        "        # Запуск игры между двумя агентами\n",
        "        env.reset()\n",
        "        env.run([agents[i], agents[j]])\n",
        "        print(f\"Игра {i+1} (Agent {i+1} против Agent {j+1})\")\n",
        "\n",
        "        # Визуализируем результаты каждой игры\n",
        "        env.render(mode=\"ipython\")\n"
      ],
      "metadata": {
        "id": "bQTNMtHpxkGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Камень-ножницы-бумага\" - игра между простыми ботами и ML-ботом**"
      ],
      "metadata": {
        "id": "rDmb5HMR7WzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Простые агенты: всегда выбирают камень, ножницы или бумагу\n",
        "def rock_agent(observation, configuration):\n",
        "    return 0  # Всегда выбирает камень\n",
        "\n",
        "def scissors_agent(observation, configuration):\n",
        "    return 2  # Всегда выбирает ножницы\n",
        "\n",
        "def paper_agent(observation, configuration):\n",
        "    return 1  # Всегда выбирает бумагу\n",
        "\n",
        "# Агент, который делает случайный выбор\n",
        "def random_agent(observation, configuration):\n",
        "    return random.randint(0, 2)\n",
        "\n",
        "# Простой ML-агент, который отслеживает частоту ходов противника и использует это для выбора\n",
        "class SimpleMLAgent:\n",
        "    def __init__(self):\n",
        "        self.opponent_actions = []  # Хранение ходов противника\n",
        "\n",
        "    def __call__(self, observation, configuration):\n",
        "        if observation.step > 0:\n",
        "            # Добавляем последний ход противника в список\n",
        "            self.opponent_actions.append(observation.lastOpponentAction)\n",
        "\n",
        "        if len(self.opponent_actions) == 0:\n",
        "            # Если это первый ход, выбираем случайно\n",
        "            return random.randint(0, 2)\n",
        "\n",
        "        # Анализируем частоту ходов противника\n",
        "        action_counts = Counter(self.opponent_actions)\n",
        "        most_common_action = action_counts.most_common(1)[0][0]  # Самый частый ход противника\n",
        "\n",
        "        # Возвращаем ход, который победит самый частый ход противника\n",
        "        return (most_common_action + 1) % 3\n",
        "\n",
        "# Создаем агента\n",
        "ml_agent = SimpleMLAgent()\n",
        "\n",
        "# Пример запуска игры между ML-агентом и случайным агентом\n",
        "from kaggle_environments import make\n",
        "\n",
        "# Создаем окружение для игры\n",
        "env = make(\"rps\", configuration={\"episodeSteps\": 100})\n",
        "\n",
        "# Запуск игры между двумя агентами\n",
        "env.run([ml_agent, random_agent])\n",
        "\n",
        "# Визуализация результатов\n",
        "env.render(mode=\"ipython\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gGKMpziGxozn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Визуализация: \"Камень-ножницы-бумага\" - Простые боты + ML-бот (pandas, seaborn, matplotlib)**"
      ],
      "metadata": {
        "id": "Gn_jr7no7jbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Простые агенты и ML-агент\n",
        "def rock_agent(observation, configuration):\n",
        "    return 0\n",
        "\n",
        "def random_agent(observation, configuration):\n",
        "    return random.randint(0, 2)\n",
        "\n",
        "class SimpleMLAgent:\n",
        "    def __init__(self):\n",
        "        self.opponent_actions = []\n",
        "\n",
        "    def __call__(self, observation, configuration):\n",
        "        if observation.step > 0:\n",
        "            self.opponent_actions.append(observation.lastOpponentAction)\n",
        "\n",
        "        if len(self.opponent_actions) == 0:\n",
        "            return random.randint(0, 2)\n",
        "\n",
        "        action_counts = Counter(self.opponent_actions)\n",
        "        most_common_action = action_counts.most_common(1)[0][0]\n",
        "        return (most_common_action + 1) % 3\n",
        "\n",
        "ml_agent = SimpleMLAgent()\n",
        "\n",
        "# Создаем окружение для игры\n",
        "from kaggle_environments import make\n",
        "env = make(\"rps\", configuration={\"episodeSteps\": 100})\n",
        "\n",
        "# Список для хранения выборов каждого агента\n",
        "choices = []\n",
        "\n",
        "# Запуск нескольких игр между агентами и сбор данных о выборе\n",
        "for _ in range(50):  # 50 игр для примера\n",
        "    env.reset()\n",
        "    game = env.run([ml_agent, random_agent])\n",
        "\n",
        "    for step in game:\n",
        "        ml_choice = step[0]['action']  # Выбор ML-агента\n",
        "        random_choice = step[1]['action']  # Выбор случайного агента\n",
        "        choices.append({'Agent': 'ML Agent', 'Choice': ml_choice})\n",
        "        choices.append({'Agent': 'Random Agent', 'Choice': random_choice})\n",
        "\n",
        "# Преобразуем данные в DataFrame для визуализации\n",
        "df_choices = pd.DataFrame(choices)\n",
        "\n",
        "# Заменим числовые значения действий на их текстовые обозначения\n",
        "df_choices['Choice'] = df_choices['Choice'].replace({0: 'Rock', 1: 'Paper', 2: 'Scissors'})\n",
        "\n",
        "# Визуализация распределения выборов каждого агента\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Choice', hue='Agent', data=df_choices)\n",
        "plt.title('Распределение выборов действий (камень, ножницы, бумага)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Сбор данных: Во время каждой игры мы собираем данные о том, какое действие (камень, бумага, ножницы) выбрал каждый агент на каждом шаге.\n",
        "# Визуализация: Строим столбчатую диаграмму, показывающую, как часто каждый агент выбирал камень, бумагу или ножницы\n",
        "# Это позволит увидеть, предпочитают ли агенты определённые действия и насколько они отличаются в стратегиях."
      ],
      "metadata": {
        "id": "bYQZg7R06OrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}